
import pandas as pd
import re
import emoji
import streamlit as st
import os
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_community.vectorstores import FAISS
from src.config import review_faiss 

def clean_review(text):
    text = str(text) 
    text = re.sub(r'\s+', ' ', text)
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
    text = text.strip()
    return text

def chunk_text_with_overlap(text, chunk_size=500, overlap=50):
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
        if start < 0: start = 0
        if start >= len(text): break
    return chunks

def find_address_from_db(db, place_name):
    """
    ê¸°ì¡´ FAISS DBì—ì„œ ì¥ì†Œëª…ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 'ìƒì„¸ ì£¼ì†Œ'ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    if not db: return ""
    
    try:
        results = db.similarity_search(place_name, k=1)
        if results:
            doc = results[0]
   
            existing_name = doc.metadata.get("ì¥ì†Œëª…", "")
            
            if place_name in existing_name or existing_name in place_name:
                address = doc.metadata.get("ìƒì„¸ ì£¼ì†Œ", "")
                if address:
                    print(f" '{place_name}'ì˜ ì£¼ì†Œë¥¼ DBì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤: {address}")
                    return address
    except Exception as e:
        print(f" ì£¼ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return ""

def create_documents_from_df(df, existing_db=None):
    """
    DataFrame -> Document ë³€í™˜
    * existing_db: ì£¼ì†Œ ì¡°íšŒë¥¼ ìœ„í•´ ì „ë‹¬ë°›ì€ ê¸°ì¡´ FAISS DB ê°ì²´
    """
    docs = []
    for _, row in df.iterrows():
        cleaned_review = clean_review(row.get("ë¦¬ë·°", "")) 
        chunks = chunk_text_with_overlap(cleaned_review, chunk_size=500, overlap=20)
        
        place_name = row.get("ì¥ì†Œëª…") if pd.notna(row.get("ì¥ì†Œëª…")) else row.get("ì¥ì†Œ", "ì¥ì†Œë¯¸ìƒ")
        category = row.get("ì¹´í…Œê³ ë¦¬_í†µí•©") if pd.notna(row.get("ì¹´í…Œê³ ë¦¬_í†µí•©")) else row.get("ì¹´í…Œê³ ë¦¬", "ê¸°íƒ€")
        rating = row.get("í‰ì ") if pd.notna(row.get("í‰ì ")) else row.get("ë³„ì ", "0")
        
    
        address = row.get("ìƒì„¸ ì£¼ì†Œ") if pd.notna(row.get("ìƒì„¸ ì£¼ì†Œ")) else ""
        
        if not address and existing_db:
            address = find_address_from_db(existing_db, place_name)

        for chunk in chunks:
            if len(chunk) <= 5: continue

            combined_text = (
                f"ì§€ì—­: {row.get('ì§€ì—­', '')} | "
                f"ì¥ì†Œëª…: {place_name} | "
                f"ì¹´í…Œê³ ë¦¬: {category} | "
                f"ë¦¬ë·°: {chunk}"
            )
            
            doc = Document(
                page_content=combined_text,
                metadata={
                    "ì§€ì—­": str(row.get("ì§€ì—­", "")),
                    "ì¹´í…Œê³ ë¦¬": str(category),
                    "ì¥ì†Œëª…": str(place_name),
                    "ë³„ì ": str(rating),
                    "ìƒì„¸ ì£¼ì†Œ": str(address),  
                    "ë¦¬ë·°": str(row.get("ë¦¬ë·°", "")[:100])
                }
            )
            docs.append(doc)
    return docs

def update_vector_db_if_needed(new_reviews_file="new_reviews.csv"):
    try:
        df = pd.read_csv(new_reviews_file)
    except (FileNotFoundError, pd.errors.EmptyDataError):
        return "ì—…ë°ì´íŠ¸í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤."

    if len(df) < 10:
        return f"ë¦¬ë·° {len(df)}ê°œ ëˆ„ì ë¨. (10ê°œ ì´ìƒì´ì–´ì•¼ ì—…ë°ì´íŠ¸)"

    st.toast(f"ë¦¬ë·° {len(df)}ê°œ DB ì—…ë°ì´íŠ¸ ì‹œì‘...")
    print(f"---  ë¦¬ë·° {len(df)}ê°œ DB ì—…ë°ì´íŠ¸ ì‹œì‘ ---")

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="upskyy/bge-m3-korean",
            model_kwargs={"device": "cpu"}
        )
        
        existing_db = None
        if os.path.exists(review_faiss):
            try:
                existing_db = FAISS.load_local(
                    review_faiss, embeddings, allow_dangerous_deserialization=True
                )
                print("ê¸°ì¡´ DB ë¡œë“œ ì™„ë£Œ (ì£¼ì†Œ ê²€ìƒ‰ìš©)")
            except Exception as e:
                print(f"ê¸°ì¡´ DB ë¡œë“œ ì‹¤íŒ¨: {e}")

        new_docs = create_documents_from_df(df, existing_db=existing_db)
        
        if not new_docs:
            os.remove(new_reviews_file) 
            return "ìœ íš¨í•œ ë¬¸ì„œ ì—†ìŒ"

        print(f"{len(new_docs)}ê°œì˜ ìƒˆ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")

        if existing_db:
            existing_db.add_documents(new_docs)
            db_to_save = existing_db
        else:
            print("ê¸°ì¡´ DBê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            db_to_save = FAISS.from_documents(new_docs, embeddings)

        db_to_save.save_local(review_faiss)
        st.cache_resource.clear()
        os.remove(new_reviews_file)
        
        print("ì—…ë°ì´íŠ¸ ì™„ë£Œ ë° ì €ì¥ë¨.")
        st.toast("ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!", icon="ğŸ‰")
        return "ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!"

    except Exception as e:
        print(f" Critical Error: {e}")
        return f"ì˜¤ë¥˜: {e}"